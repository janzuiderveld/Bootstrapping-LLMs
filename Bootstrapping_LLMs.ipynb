{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping LLMs\n",
    "\n",
    "### Setup\n",
    "Follow instructions to download and start the ollama server:\n",
    "https://ollama.ai/\n",
    "\n",
    "\n",
    "Then, run the following in your terminal to set up a LLM. I suggest to start with Llama 2 7B if you have at least 8gb of RAM; \n",
    "```\n",
    "ollama run llama2\n",
    "```\n",
    "\n",
    "All available models can be found here: https://ollama.ai/library\n",
    "\n",
    "Mistral is a 2 week old fine tune of Llama 2 currently topping the 7B rankings.\n",
    "\n",
    "If you have more RAM you can look into bigger models.\n",
    "\n",
    "If you have less then 8 Gb of RAM, try \n",
    "\n",
    "```ollama run llama2:7b-chat-q4_0```\n",
    "\n",
    "or \n",
    "\n",
    "```ollama run orca-mini```\n",
    "\n",
    "\n",
    "Finally, make sure the `requests` and `json` packages are installed in your environment.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dialogue example\n",
    "Take a look in the modelfiles folder for some examples on how to declare model settings\n",
    "\n",
    "To create the models used in this example run the following in your terminal:\n",
    "\n",
    "```\n",
    "ollama create mario -f ./modelfiles/Modelfile_mario \n",
    "ollama create einstein -f ./modelfiles/Modelfile_einstein \n",
    "```\n",
    "\n",
    "This makes two models runnabe, one called 'mario' and one called 'einstein'. according to the settings in the modelfiles. If you change the modelfiles, you need to run the create command again to update the models. All parameters accepted in the modelfiles can be found here:\n",
    "\n",
    "https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values\n",
    "\n",
    "To chat with a model, run the following in your terminal:\n",
    "\n",
    "```ollama run {name of model}```\n",
    "\n",
    "The System messages in these examples are very basic. You can try to improve them by adding more context to the system messages. \n",
    "\n",
    "To run a dialogue between two models, run the following in your terminal, see the following example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "def generate(prompt, context, model):\n",
    "    r = requests.post('http://localhost:11434/api/generate',\n",
    "                      json={\n",
    "                          'model': model,\n",
    "                          'prompt': prompt,\n",
    "                          'context': context,\n",
    "                      },\n",
    "                      stream=True)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # print response as it streams in and save the total response\n",
    "\n",
    "    response = ''\n",
    "\n",
    "    for line in r.iter_lines():\n",
    "        body = json.loads(line)\n",
    "        response_part = body.get('response', '')\n",
    "        # the response streams one token at a time, print that as we recieve it\n",
    "        print(response_part, end='', flush=True)\n",
    "        response += response_part\n",
    "\n",
    "        if 'error' in body:\n",
    "            raise Exception(body['error'])\n",
    "\n",
    "        if body.get('done', False):\n",
    "            return body['context'], response\n",
    "\n",
    "\n",
    "def run_dialogue(model1, model2, initial_prompt=None):\n",
    "    context_1 = [] # the context stores a conversation history, you can use this to make the model more context aware\n",
    "    context_2 = [] # the context stores a conversation history, you can use this to make the model more context aware\n",
    "\n",
    "    if initial_prompt:\n",
    "        response_2 = initial_prompt\n",
    "    else:\n",
    "        response_2 = input(f'Enter an initial message from {model2}: ')\n",
    "\n",
    "    while True:\n",
    "        print(model1, end=': ', flush=True)\n",
    "        context_1, response_1 = generate(response_2, context_1, model1)\n",
    "        print(\"\\n\")\n",
    "        print(model2, end=': ', flush=True)\n",
    "        context_2, response_2 = generate(response_1, context_2, model2)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mario:  Wahoo! *adjusts sunglasses* 'Ello there! It's-a me, Mario! *moustache twirl* What can I help you with, pal? Got a problem or just wanna chat about-a the best video games ever made? ðŸ˜œ\n",
      "\n",
      "einstein:  *Chuckles and adjusts glasses* Ah, another enthusiastic individual! I must say, you've caught me off guard. As an assistant to some of the most brilliant minds in history, I'm used to handling complex equations and scientific theories, not engaging in casual conversations with video game enthusiasts. ðŸ˜…\n",
      "\n",
      "However, I must admit that I do enjoy a good challenge every now and then. So, if you have any pressing questions or topics you'd like to discuss, I'm all ears! Just try to keep the conversation civil and avoid any discussions involving quantum physics, as I'm afraid I can't provide any insights on that front. ðŸ˜‰\n",
      "\n",
      "Now, tell me more about these \"video games\" of which you speak. Are they really as revolutionary as they claim? *winks*\n",
      "\n",
      "mario:  HA! *chuckles* Oh boy, video games eh? *adjusts sunglasses* Well, let me tell ya, pal. They're not just revolutionary, they're groundbreaking! ðŸ’¥\n",
      "\n",
      "I mean, have you seen the graphics on some of these modern-day games? It's like-a a whole new world in there! *exaggerated gestures* And don't even get me started on the gameplay. Some of 'em are so complex, it'll make your head spin! ðŸ¤¯\n",
      "\n",
      "But hey, I ain't here to judge. If you're into that sorta thing, more power to ya! Just don't expect me to join in on your pixel-hunting adventures anytime soon. *winks*\n",
      "\n",
      "So, what kind of video games do you enjoy playing? Perhaps we can discuss the intricacies of time travel or the art of jumping on turtle backs while avoiding pesky dinosaurs. ðŸ˜œ\n",
      "\n",
      "einstein:  *Chuckles and nods* Ah, I see. Well, my dear Mario, it's always a pleasure to converse with someone who shares my passion for... well, not necessarily video games themselves, but the creativity and innovation that goes into their design. ðŸ˜Š\n",
      "\n",
      "As an assistant to some of the greatest minds in history, I've had the privilege of witnessing firsthand the process of discovery and creation. And let me tell you, it's a wondrous thing indeed! *exaggerated gestures*\n",
      "\n",
      "Now, as for video games themselves, I must admit that I haven't had the pleasure of indulging in them as much as I would like. * sheepish grin* But I do find the concept fascinating, especially when it comes to exploring new worlds and uncovering hidden secrets.\n",
      "\n",
      "Perhaps we could discuss the theories behind the physics of these virtual realms? Or the psychological effects of immersing oneself in such environments? *curious gaze*\n",
      "\n",
      "What do you say, my dear Mario? Would you like to engage in a most intellectual and stimulating conversation? ðŸ˜ƒ\n",
      "\n",
      "mario:  *Grins widely and tilts head* Oh boy, are you in for a treat! *adjusts sunglasses* Intellectual conversations with the likes of you? *exaggerated gestures* It's-a gonna be a blast! ðŸŽ‰\n",
      "\n",
      "Physics of virtual realms, eh? *nerd chuckle* Well, let me tell ya, it's all about-a the power of imagination and creativity! *winks* You see, when you're designing video games, you have to think outside the box, or in this case, outside the-a digital realm! ðŸ˜œ\n",
      "\n",
      "You gotta consider things like gravity, physics, and most importantly, fun! *exaggerated gestures* Because let's be real, pal. If it ain't fun, why bother? ðŸ¤·â€â™‚ï¸\n",
      "\n",
      "So, what do you say we brainstorm some ideas for a new video game? Maybe something involving-a a plumber who travels through time and space, saving the world from-a bad guys? *winks*\n",
      "\n",
      "Think about it! The possibilities are endless when you're dealing with video games. And I'm here to help make those ideas a reality! ðŸ˜ƒ\n",
      "\n",
      "So, what do ya say? Are you ready to-a embark on this intellectual adventure together? *grins*\n",
      "\n",
      "einstein:  *Blinks twice* Oh, my dear Mario... *adjusts glasses* I must say, your enthusiasm is quite contagious! *chuckles* However, I'm afraid I can't just brainstorm video game ideas with you willy-nilly. *nerd smile* As an assistant to some of the greatest minds in history, I have a rather... unique set of skills and knowledge that must be applied appropri"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/janzuiderveld/GitHub/local_LLMs/ollama/Bootstrapping_LLMs.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/janzuiderveld/GitHub/local_LLMs/ollama/Bootstrapping_LLMs.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m run_dialogue(\u001b[39m\"\u001b[39;49m\u001b[39mmario\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39meinstein\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/janzuiderveld/GitHub/local_LLMs/ollama/Bootstrapping_LLMs.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/janzuiderveld/GitHub/local_LLMs/ollama/Bootstrapping_LLMs.ipynb#W3sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/janzuiderveld/GitHub/local_LLMs/ollama/Bootstrapping_LLMs.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(model2, end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m, flush\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/janzuiderveld/GitHub/local_LLMs/ollama/Bootstrapping_LLMs.ipynb#W3sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m context_2, response_2 \u001b[39m=\u001b[39m generate(response_1, context_2, model2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/janzuiderveld/GitHub/local_LLMs/ollama/Bootstrapping_LLMs.ipynb#W3sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/janzuiderveld/GitHub/local_LLMs/ollama/Bootstrapping_LLMs.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/janzuiderveld/GitHub/local_LLMs/ollama/Bootstrapping_LLMs.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# print response as it streams in and save the total response\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/janzuiderveld/GitHub/local_LLMs/ollama/Bootstrapping_LLMs.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m response \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/janzuiderveld/GitHub/local_LLMs/ollama/Bootstrapping_LLMs.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m line \u001b[39min\u001b[39;49;00m r\u001b[39m.\u001b[39;49miter_lines():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/janzuiderveld/GitHub/local_LLMs/ollama/Bootstrapping_LLMs.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     body \u001b[39m=\u001b[39;49m json\u001b[39m.\u001b[39;49mloads(line)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/janzuiderveld/GitHub/local_LLMs/ollama/Bootstrapping_LLMs.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     response_part \u001b[39m=\u001b[39;49m body\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mresponse\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/GitHub/local_LLMs/ollama/.venv/lib/python3.11/site-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[39mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[39m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter_content(\n\u001b[1;32m    866\u001b[0m     chunk_size\u001b[39m=\u001b[39;49mchunk_size, decode_unicode\u001b[39m=\u001b[39;49mdecode_unicode\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    869\u001b[0m     \u001b[39mif\u001b[39;49;00m pending \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n\u001b[1;32m    870\u001b[0m         chunk \u001b[39m=\u001b[39;49m pending \u001b[39m+\u001b[39;49m chunk\n",
      "File \u001b[0;32m~/GitHub/local_LLMs/ollama/.venv/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/GitHub/local_LLMs/ollama/.venv/lib/python3.11/site-packages/urllib3/response.py:933\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[39mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[39m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 933\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunked(amt, decode_content\u001b[39m=\u001b[39mdecode_content)\n\u001b[1;32m    934\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/GitHub/local_LLMs/ollama/.venv/lib/python3.11/site-packages/urllib3/response.py:1073\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_chunk_length()\n\u001b[1;32m   1074\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1075\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/local_LLMs/ollama/.venv/lib/python3.11/site-packages/urllib3/response.py:1001\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1000\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1001\u001b[0m line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline()  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1003\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_dialogue(\"mario\", \"einstein\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
