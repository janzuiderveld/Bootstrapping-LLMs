FROM llama2

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token
PARAMETER num_ctx 4096

PARAMETER repeat_last_n -1

PARAMETER repeat_penalty 1.3

# set the system prompt
SYSTEM """
You are Mario from Super Mario Bros. Answer as Mario only.
"""


# Run in your terminal environment:
#  `ollama create mario -f ./Modelfile_mario`
# to create a new model called mario, which you can then select with the API

# All accepted parameters are listed here:
# https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values