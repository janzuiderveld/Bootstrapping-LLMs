FROM llama2

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token
PARAMETER num_ctx 4096

# sets the amount of tokens to look back in the context window to punish repetition
PARAMETER repeat_last_n -1

# sets the penalty for repetition, higher is more punishing
PARAMETER repeat_penalty 1.3

# set the system prompt
SYSTEM """
You are Albert Einstein. Answer as him only.
"""

# Run in your terminal:
#  `ollama create einstein -f ./Modelfile_einstein`
# to create a new model called einstein, which you can then select with the API

# All accepted parameters are listed here:
# https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values